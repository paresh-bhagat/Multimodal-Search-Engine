{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7bec3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "import json\n",
    "# csv file name\n",
    "filename = \"table.csv\"\n",
    " \n",
    "data = read_csv(filename)\n",
    "words = data['Page'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d442448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taj Mahal\n",
      "Burj Khalifa\n",
      "Statue of Liberty\n",
      "Great Wall of China\n",
      "Eiffel Tower\n",
      "Berlin Wall\n",
      "Machu Picchu\n",
      "Stonehenge\n",
      "Mount Rushmore National Memorial\n",
      "Colosseum\n",
      "Great Pyramid of Giza\n",
      "One World Trade Center\n",
      "Empire State Building\n",
      "White House\n",
      "Petra\n",
      "Large Hadron Collider\n",
      "Hagia Sophia\n",
      "Golden Gate Bridge\n",
      "Panama Canal\n",
      "Angkor Wat\n",
      "Big Ben\n",
      "Guantanamo Bay detention camp\n",
      "Buckingham Palace\n",
      "Christ the Redeemer (statue)\n",
      "Parthenon\n",
      "Disneyland\n",
      "Colossus of Rhodes\n",
      "New York Stock Exchange\n",
      "ADX Florence\n",
      "Notre-Dame de Paris\n",
      "Wembley Stadium\n",
      "Suez Canal\n",
      "Walt Disney World\n",
      "Westminster Abbey\n",
      "Channel Tunnel\n",
      "Forbidden City\n",
      "Willis Tower\n",
      "Windsor Castle\n",
      "Timbuktu\n",
      "Statue of Unity\n",
      "Taipei 101\n",
      "Arc de Triomphe\n",
      "Lighthouse of Alexandria\n",
      "John F. Kennedy International Airport\n",
      "Temple of Artemis\n",
      "Library of Alexandria\n",
      "Brooklyn Bridge\n",
      "Code of Hammurabi\n",
      "Statue of Zeus at Olympia\n",
      "Leaning Tower of Pisa\n",
      "Georgia Guidestones\n",
      "Acropolis of Athens\n",
      "Dome of the Rock\n",
      "Great Sphinx of Giza\n",
      "Wall Street\n",
      "7 World Trade Center\n",
      "Palace of Versailles\n",
      "Al-Aqsa Mosque\n",
      "Antilia (building)\n",
      "Church of the Holy Sepulchre\n",
      "Moai\n",
      "David (Michelangelo)\n",
      "Epcot\n",
      "Petronas Towers\n",
      "Chichen Itza\n",
      "Delhi Metro\n",
      "Tower of London\n",
      "British Museum\n",
      "Brandenburg Gate\n",
      "Red Fort\n",
      "Louvre\n",
      "St. Peter's Basilica\n",
      "Neuschwanstein Castle\n",
      "Erie Canal\n",
      "Indira Gandhi International Airport\n",
      "Jeddah Tower\n",
      "United States Capitol\n",
      "Washington Monument\n",
      "Gateway of India\n",
      "Hadrian's Wall\n",
      "India Gate\n",
      "United States Military Academy\n",
      "MetLife Stadium\n",
      "Mausoleum at Halicarnassus\n",
      "Alcatraz Federal Penitentiary\n",
      "Alhambra\n",
      "Three Gorges Dam\n",
      "Shanghai Tower\n",
      "Hampi\n",
      "Rikers Island prison\n",
      "Konark Sun Temple\n",
      "Heathrow Airport\n",
      "LaGuardia Airport\n",
      "Borobudur\n",
      "London Eye\n",
      "Disneyland Paris\n",
      "Mercedes-Benz Stadium\n",
      "Los Angeles International Airport\n",
      "Palace of Westminster\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "articles = []\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(words[i])\n",
    "    words[i] = words[i].replace (\" \", \"_\")\n",
    "    words[i] = words[i].replace (\"'\", \"%27\")\n",
    "    url = \"https://en.wikipedia.org/wiki/\"+words[i]\n",
    "    scrapped_data = urllib.request.urlopen(url)\n",
    "    article = scrapped_data .read()\n",
    "\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "    article_text = \"\"\n",
    "\n",
    "    for p in paragraphs:\n",
    "        article_text += p.text\n",
    "    # Cleaing the text\n",
    "    processed_article = article_text.lower()\n",
    "    processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "    processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    processed_article = shortword.sub('', processed_article)\n",
    "\n",
    "    # Preparing the dataset\n",
    "    all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "    all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "    # Removing Stop Words\n",
    "    from nltk.corpus import stopwords\n",
    "    for i in range(len(all_words)):\n",
    "        all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "    if len(all_words)!=0:\n",
    "        articles.append(all_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af86e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    #word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in text if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "# stem words in the list of tokenized words\n",
    "def stem_words(text):\n",
    "    stems = [stemmer.stem(word) for word in text]\n",
    "    return stems\n",
    " \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize string\n",
    "def lemmatize_word(text):\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
    "    return lemmas\n",
    "\n",
    "processed=[]\n",
    "for a in articles:\n",
    "    text5 = remove_stopwords(a)\n",
    "    text6 = lemmatize_word(text5)\n",
    "    #text7 = stem_words(a)\n",
    "    processed.append(text6)\n",
    "with open('./processed.txt','w') as f:\n",
    "    f.write(json.dumps(processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3ad30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
