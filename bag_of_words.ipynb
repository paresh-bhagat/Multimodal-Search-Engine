{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92757e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "## extract the descriptors from all images. ##\n",
    "sift = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb3450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(centroids, img_descriptors):\n",
    "    n_centroids = centroids.shape[0]  # number of centroids found with the KMeans clustering #100\n",
    "    n_descriptors = img_descriptors.shape[0]  # number of descriptors extracted from the image #200\n",
    "    \n",
    "    # initialization of the bag of words (BoW) vector\n",
    "    # Note that the BoW vector has length equal to the number of cluster centroids\n",
    "    # The cluster centroids are indeed our visual words, and the BoW will be the histogram of these words found in the given image\n",
    "    bow_vector = np.zeros(n_centroids)  \n",
    "    \n",
    "    for i in range(n_descriptors):\n",
    "        for j in range(n_centroids):\n",
    "            if img_descriptors[i][j]==True: #if the feature is in the image (true in img_descriptor)\n",
    "                bow_vector[j]+=1            #bow_vector.shape => 100 (for each image)\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a991c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## RETRIEVE IMAGES ##\n",
    "\n",
    "def retrieve_images(map_bow_vectors, query_bow):\n",
    "    n_map_bow_vectors = map_bow_vectors.shape[0]\n",
    "    bow_distances = np.zeros(n_map_bow_vectors)\n",
    "    most_similar = None  \n",
    "    \n",
    "    for i in range(n_map_bow_vectors):\n",
    "        for j in range(len(query_bow)):\n",
    "            \n",
    "            bow_distances[i] += distance.euclidean(map_bow_vectors[i][j], query_bow[j])        \n",
    "            #bow_distances[i] +=distance.minkowski(map_bow_vectors[i][j], query_bow[j])\n",
    "            #bow_distances[i] += distance.cosine(map_bow_vectors[i][j], query_bow[j])\n",
    "            #bow_distances[i] += distance.cityblock(map_bow_vectors[i][j], query_bow[j]) #manhattan\n",
    "              \n",
    "    most_similar=np.argsort(bow_distances)\n",
    "    most_similar=most_similar[:10]\n",
    "        \n",
    "    return most_similar\n",
    "\n",
    "\n",
    "# Retrieve the most similar images to query image 87 (index 87-1=86)\n",
    "def search_bagofword(image_path):\n",
    "    start = time.time()\n",
    "    bow_map_images = np.load(\"./bag_of_words/bow_map_images.npy\")\n",
    "    centroids = np.load(\"./bag_of_words/centroids.npy\")\n",
    "    f = open('./bag_of_words/mylist1.txt','r')\n",
    "    names = json.loads(f.read())\n",
    "    f.close()\n",
    "    f = open('./bag_of_words/mylist2.txt','r')\n",
    "    labels = json.loads(f.read())\n",
    "    f.close()\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    (kps, query_img_descriptors) = sift.detectAndCompute(gray, None)\n",
    "    \n",
    "    with open(\"./bag_of_words/kmeanmodel.pkl\",\"rb\") as f:\n",
    "        kmeans2=pickle.load(f)\n",
    "    \n",
    "    bow = bag_of_words(centroids, query_img_descriptors)\n",
    "\n",
    "    # Retrieve the indices of the top-10 similar images from the map\n",
    "    retrieved_images = retrieve_images(bow_map_images, bow)\n",
    "\n",
    "    #print('Indices of similar images retrieved: ', retrieved_images)\n",
    "    # Indices of the relevant map images for the query: we have the relevance judgements (Ground truth)\n",
    "    #relevant_images = np.where(sim[86, :] == 1)[0]\n",
    "    #print('Indices of relevant images (given in the GT relevance judgements): ', relevant_images)\n",
    "    \n",
    "    s_img = image_path\n",
    "    s_image = Image.open(s_img)\n",
    "    \n",
    "    with open('./search_results/result1.jpg', 'wb') as f:\n",
    "        s_image.save(f, \"JPEG\", quality=85)\n",
    "    \n",
    "    s1 = \"images/\" + names[retrieved_images[0]]\n",
    "    \n",
    "    s_img = s1\n",
    "    s_image = Image.open(s_img)\n",
    "    \n",
    "    with open('./search_results/result2.jpg', 'wb') as f:\n",
    "        s_image.save(f, \"JPEG\", quality=85)\n",
    "    \n",
    "    s2 = \"images/\" + names[retrieved_images[1]]\n",
    "    \n",
    "    s_img = s2\n",
    "    s_image = Image.open(s_img)\n",
    "    \n",
    "    with open('./search_results/result3.jpg', 'wb') as f:\n",
    "        s_image.save(f, \"JPEG\", quality=85)\n",
    "    \n",
    "    end = time.time()\n",
    "        \n",
    "    plt.figure(figsize=(17,10))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title('queued')\n",
    "    plt.imshow(plt.imread(image_path))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title('first retieved-'+labels[names[retrieved_images[0]]])\n",
    "    plt.imshow(plt.imread(s1))\n",
    "    plt.axis(\"off\")\n",
    "        \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title('second retieved-'+labels[names[retrieved_images[1]]])\n",
    "    plt.imshow(plt.imread(s2))\n",
    "    plt.axis(\"off\")\n",
    "        \n",
    "    print()\n",
    "    print(\"time taken to search:\",round(end-start,1),\"sec\")\n",
    "    list2 = []\n",
    "    list2.append(labels[names[retrieved_images[0]]])\n",
    "    list2.append(labels[names[retrieved_images[1]]])\n",
    "    list2.append(\"Time taken to search : \"+ str(round(end-start,1)) + \" sec\")\n",
    "    return list2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df1ebe",
   "metadata": {},
   "source": [
    "## search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8395806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_image(image_path):\n",
    "    img = image_path\n",
    "    image = Image.open(img)\n",
    "    width = image.width\n",
    "    height = image.height\n",
    "\n",
    "    if width>300:\n",
    "        basewidth = 300\n",
    "        wpercent = (basewidth / float(image.size[0]))\n",
    "        hsize = int((float(image.size[1]) * float(wpercent)))\n",
    "        image = image.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "            \n",
    "    if height>300:\n",
    "        baseheight = 300\n",
    "        hpercent = (baseheight / float(image.size[1]))\n",
    "        wsize = int((float(image.size[0]) * float(hpercent)))\n",
    "        image = image.resize((wsize, baseheight), Image.ANTIALIAS)\n",
    "\n",
    "    with open('./search_images/temp.jpg', 'wb') as f:\n",
    "        image.save(f, \"JPEG\", quality=85)\n",
    "    \n",
    "    dmlist = search_bagofword('./search_images/temp.jpg')\n",
    "    return dmlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4b0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dm = search_image('./search_images/images8.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4247af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
