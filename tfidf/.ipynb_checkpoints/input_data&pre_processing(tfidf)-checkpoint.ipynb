{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58ebd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taj Mahal',\n",
       " 'Burj Khalifa',\n",
       " 'Statue of Liberty',\n",
       " 'Great Wall of China',\n",
       " 'Eiffel Tower',\n",
       " 'Berlin Wall',\n",
       " 'Machu Picchu',\n",
       " 'Stonehenge',\n",
       " 'Mount Rushmore National Memorial',\n",
       " 'Colosseum',\n",
       " 'Great Pyramid of Giza',\n",
       " 'One World Trade Center',\n",
       " 'Empire State Building',\n",
       " 'White House',\n",
       " 'Petra',\n",
       " 'Large Hadron Collider',\n",
       " 'Hagia Sophia',\n",
       " 'Golden Gate Bridge',\n",
       " 'Panama Canal',\n",
       " 'Angkor Wat',\n",
       " 'Big Ben',\n",
       " 'Guantanamo Bay detention camp',\n",
       " 'Buckingham Palace',\n",
       " 'Christ the Redeemer (statue)',\n",
       " 'Parthenon',\n",
       " 'Disneyland',\n",
       " 'Colossus of Rhodes',\n",
       " 'New York Stock Exchange',\n",
       " 'ADX Florence',\n",
       " 'Notre-Dame de Paris',\n",
       " 'Wembley Stadium',\n",
       " 'Suez Canal',\n",
       " 'Walt Disney World',\n",
       " 'Westminster Abbey',\n",
       " 'Channel Tunnel',\n",
       " 'Forbidden City',\n",
       " 'Willis Tower',\n",
       " 'Windsor Castle',\n",
       " 'Timbuktu',\n",
       " 'Statue of Unity',\n",
       " 'Taipei 101',\n",
       " 'Arc de Triomphe',\n",
       " 'Lighthouse of Alexandria',\n",
       " 'John F. Kennedy International Airport',\n",
       " 'Temple of Artemis',\n",
       " 'Library of Alexandria',\n",
       " 'Brooklyn Bridge',\n",
       " 'Code of Hammurabi',\n",
       " 'Statue of Zeus at Olympia',\n",
       " 'Leaning Tower of Pisa',\n",
       " 'Georgia Guidestones',\n",
       " 'Acropolis of Athens',\n",
       " 'Dome of the Rock',\n",
       " 'Great Sphinx of Giza',\n",
       " 'Wall Street',\n",
       " '7 World Trade Center',\n",
       " 'Palace of Versailles',\n",
       " 'Al-Aqsa Mosque',\n",
       " 'Antilia (building)',\n",
       " 'Church of the Holy Sepulchre',\n",
       " 'Moai',\n",
       " 'David (Michelangelo)',\n",
       " 'Epcot',\n",
       " 'Petronas Towers',\n",
       " 'Chichen Itza',\n",
       " 'Delhi Metro',\n",
       " 'Tower of London',\n",
       " 'British Museum',\n",
       " 'Brandenburg Gate',\n",
       " 'Red Fort',\n",
       " 'Louvre',\n",
       " \"St. Peter's Basilica\",\n",
       " 'Neuschwanstein Castle',\n",
       " 'Erie Canal',\n",
       " 'Indira Gandhi International Airport',\n",
       " 'Jeddah Tower',\n",
       " 'United States Capitol',\n",
       " 'Washington Monument',\n",
       " 'Gateway of India',\n",
       " \"Hadrian's Wall\",\n",
       " 'India Gate',\n",
       " 'United States Military Academy',\n",
       " 'MetLife Stadium',\n",
       " 'Mausoleum at Halicarnassus',\n",
       " 'Alcatraz Federal Penitentiary',\n",
       " 'Alhambra',\n",
       " 'Three Gorges Dam',\n",
       " 'Shanghai Tower',\n",
       " 'Hampi',\n",
       " 'Rikers Island prison',\n",
       " 'Konark Sun Temple',\n",
       " 'Heathrow Airport',\n",
       " 'LaGuardia Airport',\n",
       " 'Borobudur',\n",
       " 'London Eye',\n",
       " 'Disneyland Paris',\n",
       " 'Mercedes-Benz Stadium',\n",
       " 'Los Angeles International Airport',\n",
       " 'Palace of Westminster']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "# csv file name\n",
    "filename = \"table.csv\"\n",
    " \n",
    "data = pd.read_csv(filename)\n",
    "words = data['Page'].tolist()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7625d223",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tfidf/text_files/Taj_Mahal.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m processed_article \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, processed_article )\n\u001b[0;32m     26\u001b[0m processed_article \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, processed_article)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./tfidf/text_files/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     28\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(processed_article)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tfidf/text_files/Taj_Mahal.txt'"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "a=[]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    words[i] = words[i].replace (\" \", \"_\")\n",
    "    url = \"https://en.wikipedia.org/wiki/\"+words[i]\n",
    "    scrapped_data = urllib.request.urlopen(url)\n",
    "    article = scrapped_data .read()\n",
    "\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "    article_text = \"\"\n",
    "\n",
    "    for p in paragraphs:\n",
    "        article_text += p.text\n",
    "    # Cleaing the text\n",
    "    processed_article = article_text.lower()\n",
    "    processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "    processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "    with open('./text_files/'+words[i]+'.txt','w') as f:\n",
    "        f.write(processed_article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
